algorithm='Lloyd')$tot.withinss
}
# Create a function to calculate total intra-cluster sum of squares
iss <- functio(k){
kmeans(x = customer[, 3:5],
k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
# Create a function to calculate total intra-cluster sum of squares
iss <- functio(k){
kmeans(x = customer[, 3:5],
k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
# Create a function to calculate total intra-cluster sum of squares
iss <- functio(k) {
kmeans(x=customer[, 3:5],
k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
iss <- functio(k) {
kmeans(x=customer[, 3:5],
k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
iss <- functio(k) {
kmeans(x=customer[, 3:5],
k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
iss <- functio(k) {
kmeans(x=customer[, 3:5],
centers = k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
iss <- function(k) {
kmeans(x=customer[, 3:5],
centers = k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
# Determine the number of clusters that we want to test
k.values <- 1:10
start_time <- Sys.time()
iss_values <- map_dbl(k.values, iss)
end_time <- Sys.time()
paste("The running time was: ", end_time - start_time, sep=' ')
# Plotting the results
plot(k.values,
iss_values,
type='b', pch=19, frame=F,
xlab='Number of Clusters K',
ylab='Total Intra-clusters sum of squares')
# Plotting the results
par(mfrow=c(1,1))
plot(k.values,
iss_values,
type='b', pch=19, frame=F,
xlab='Number of Clusters K',
ylab='Total Intra-clusters sum of squares')
k <- 4
library(cluster)
library(grid)
library(gridExtra)
k2 <- kmeans(customer[, 3:5],
2,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s2 <- plot(silhouette(k2$cluster, dist(customer[, 3:5], 'euclidean')))
k3 <- kmeans(customer[, 3:5],
3,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s3 <- plot(silhouette(k2$cluster, dist(customer[, 3:5], 'euclidean')))
k4 <- kmeans(customer[, 3:5],
4,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s4 <- plot(silhouette(k2$cluster, dist(customer[, 3:5], 'euclidean')))
plot(silhouette(k2$cluster, dist(customer[, 3:5], 'euclidean')))
k3 <- kmeans(customer[, 3:5],
3,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s3 <- plot(silhouette(k3$cluster, dist(customer[, 3:5], 'euclidean')))
k4 <- kmeans(customer[, 3:5],
4,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s4 <- plot(silhouette(k4$cluster, dist(customer[, 3:5], 'euclidean')))
k5 <- kmeans(customer[, 3:5],
5,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s5 <- plot(silhouette(k5$cluster, dist(customer[, 3:5], 'euclidean')))
k10 <- kmeans(customer[, 3:5],
10,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s10 <- plot(silhouette(k10$cluster, dist(customer[, 3:5], 'euclidean')))
k5 <- kmeans(customer[, 3:5],
5,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s5 <- plot(silhouette(k5$cluster, dist(customer[, 3:5], 'euclidean')))
k6 <- kmeans(customer[, 3:5],
6,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s6 <- plot(silhouette(k6$cluster, dist(customer[, 3:5], 'euclidean')))
k7 <- kmeans(customer[, 3:5],
7,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s7 <- plot(silhouette(k7$cluster, dist(customer[, 3:5], 'euclidean')))
k8 <- kmeans(customer[, 3:5],
8,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s8 <- plot(silhouette(k8$cluster, dist(customer[, 3:5], 'euclidean')))
k9 <- kmeans(customer[, 3:5],
9,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s9 <- plot(silhouette(k9$cluster, dist(customer[, 3:5], 'euclidean')))
k10 <- kmeans(customer[, 3:5],
10,
iter.max=100,
nstart=50,
algorithm='Lloyd')
s10 <- plot(silhouette(k10$cluster, dist(customer[, 3:5], 'euclidean')))
library(NbClust)
install.package('NbClust')
install.packages('NbClust')
library(NbClust)
install.packages('factoextra')
# Execute the function to get the plot of model performance
fviz_nbclust(customer[, 3:5],
kmeans,
method='silhouette')
library(factoextra)
# Execute the function to get the plot of model performance
fviz_nbclust(customer[, 3:5],
kmeans,
method='silhouette')
library(NbClust)
stat_gap <- clusGap(customer[, 3:5],
FUN=kmeans,
nstart=25,
K.max=10, B=50)
fviz_gap_stat(stat_gap)
k6
pcclust=prcomp(customer_data[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)
pcclust$rotation[,1:2]
pcclust=prcomp(customer[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)
pcclust$rotation[,1:2]
set.seed(1)
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) +
geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c("1", "2", "3", "4", "5","6"),
labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
set.seed(1)
ggplot(customer, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) +
geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c("1", "2", "3", "4", "5","6"),
labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
# It is seems reasonable to choose K=6, as it has been suggested by 2 methods.
print(k6)
pcclust=prcomp(customer[,3:5],scale=FALSE) #principal component analysis
summary(pcclust)
pcclust$rotation[,1:2]
ggplot(customer,
aes(x=Annual.Income..k..,
y=Spending.Score..1.100.)) +
geom_point(stat='identity',
aes(color=as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c('1', '2', '3', '4', '5', '6'),
labels=c('Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6')) +
ggtitle("Segments of Mall Customers",
subtitle='Using K-means Clustering')
pcclust$rotation[,1:2]
summary(pcclust)
pcclust
pcclust$rotation
pcclust$rotation[,1:2]
# But we can explore the clustering result with the third component, the age, as well.
# Income and Age
ggplot(customer,
aes(x=Annual.Income..k..,
y=Age)) +
geom_point(stat='identity',
aes(color=as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c('1', '2', '3', '4', '5', '6'),
labels=c('Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6')) +
ggtitle("Segments of Mall Customers",
subtitle='Using K-means Clustering')
# As we can see, the age is not a good parameter for segmenting the customers of this dataset!
# Spending score and Age
ggplot(customer,
aes(x=Spending.Score..1.100.
y=Age)) +
geom_point(stat='identity',
aes(color=as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c('1', '2', '3', '4', '5', '6'),
labels=c('Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6')) +
ggtitle("Segments of Mall Customers",
subtitle='Using K-means Clustering')
# As we can see, the age is not a good parameter for segmenting the customers of this dataset!
# Spending score and Age
ggplot(customer,
aes(x=Spending.Score..1.100.,
y=Age)) +
geom_point(stat='identity',
aes(color=as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c('1', '2', '3', '4', '5', '6'),
labels=c('Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6')) +
ggtitle("Segments of Mall Customers",
subtitle='Using K-means Clustering')
# So, the annual income and spending score are the best features to segment our customers.
ggplot(customer,
aes(x=Annual.Income..k..,
y=Spending.Score..1.100.)) +
geom_point(stat='identity',
aes(color=as.factor(k6$cluster))) +
scale_color_discrete(name=" ",
breaks=c('1', '2', '3', '4', '5', '6'),
labels=c('Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6')) +
ggtitle("Segments of Mall Customers",
subtitle='Using K-means Clustering')
ggplot(customer,
aes(x=Annual.Income..k..,
y=Spending.Score..1.100.)) +
geom_point(stat='identity',
aes(color=as.factor(k5$cluster))) +
scale_color_discrete(name=" ",
breaks=c('1', '2', '3', '4', '5'),
labels=c('Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5')) +
ggtitle("Segments of Mall Customers",
subtitle='Using K-means Clustering')
k5
# It is seems reasonable to choose K=6, as it has been suggested by 2 methods.
print(k6)
stat_gap <- clusGap(customer[, 3:5],
FUN=kmeans,
nstart=25,
K.max=10, B=50)
fviz_gap_stat(stat_gap)
k5$tot.withinss
k6$tot.withinss
k5$betweenss
k6$betweenss
source("~/.active-rstudio-document", echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Plotting the income distribution by gender
par(mfrow=c(1,2))
hist(filter(customer, Gender=='Female')$Annual.Income..k..,
xlab='Annual Income (x1000)',
ylab='# of customers',
main='Income Distribution for Female',
col='steelblue')
hist(filter(customer, Gender=='Male')$Annual.Income..k..,
xlab='Annual Incone (x1000)',
ylab='# of Customers',
main='Income Distribution for Male',
col='steelblue')
# And plotting the distribution of income as function of age
customer$Gender <- as.factor(customer$Gender)
par(mfrow=c(1,1))
plot(x=customer$Age,
y=customer$Spending.Score..1.100.,
pch=16,
cex=1.5,
col=customer$Gender,
xlab='Age',
ylab='Annual Income (x1000)',
main='Income Distribution by age and sex')
legend('topright', legend=unique(customer$Gender),
col=c('black', 'red'),
pch=16)
hist(filter(customer, Gender=='Female')$Annual.Income..k..,
xlab='Annual Income (x1000)',
ylab='# of customers',
main='Income Distribution for Female',
col='steelblue',
bins=12)
hist(filter(customer, Gender=='Female')$Annual.Income..k..,
xlab='Annual Income (x1000)',
ylab='# of customers',
main='Income Distribution for Female',
col='steelblue',
breaks=12)
# Plotting the income distribution by gender
par(mfrow=c(1,2))
hist(filter(customer, Gender=='Female')$Annual.Income..k..,
xlab='Annual Income (x1000)',
ylab='# of customers',
main='Income Distribution for Female',
col='steelblue',
breaks=12)
hist(filter(customer, Gender=='Male')$Annual.Income..k..,
xlab='Annual Incone (x1000)',
ylab='# of Customers',
main='Income Distribution for Male',
col='steelblue',
breaks=12)
# And plotting the distribution of income as function of age
customer$Gender <- as.factor(customer$Gender)
par(mfrow=c(1,1))
plot(x=customer$Age,
y=customer$Spending.Score..1.100.,
pch=16,
cex=1.5,
col=customer$Gender,
xlab='Age',
ylab='Annual Income (x1000)',
main='Income Distribution by age and sex')
legend('topright', legend=unique(customer$Gender),
col=c('black', 'red'),
pch=16)
par(mfrow=c(1,2))
hist(customer$Spending.Score..1.100.,
main='Speding Score Histogram',
xlab='Speding Score',
ylab='# Of Customers',
col='steelblue')
boxplot(customer$Spending.Score..1.100. ~ customer$Gender,
main='Speding Score Distribution',
xlab='Gender',
ylab='Spending Score',
col=brewer.pal(2, 'Dark2'))
par(mfrow=c(1,2))
hist(customer$Spending.Score..1.100.,
main='Speding Score Histogram',
xlab='Speding Score',
ylab='# Of Customers',
col='steelblue')
boxplot(customer$Spending.Score..1.100. ~ customer$Gender,
main='Speding Score Distribution',
xlab='Gender',
ylab='Spending Score',
col=brewer.pal(2, 'Dark2'),
las=1)
par(mfrow=c(1,2))
hist(customer$Spending.Score..1.100.,
main='Speding Score Histogram',
xlab='Speding Score',
ylab='# Of Customers',
col='steelblue')
boxplot(customer$Spending.Score..1.100. ~ customer$Gender,
main='Speding Score Distribution',
xlab='Gender',
ylab='Spending Score',
col=brewer.pal(2, 'Dark2'))
par(mfrow=c(1,2))
hist(customer$Spending.Score..1.100.,
main='Speding Score Histogram',
xlab='Speding Score',
ylab='# Of Customers',
col='steelblue')
boxplot(customer$Spending.Score..1.100. ~ customer$Gender,
main='Speding Score Distribution',
xlab='Gender',
ylab='Spending Score',
col=brewer.pal(2, 'Dark2'),
varwidth=T)
knitr::opts_chunk$set(echo = TRUE)
genders <- table(customer$Gender)
barplot(genders,
main='Gender Distribution of Customers database',
xlab='Gender',
ylab='# of Customers',
col=brewer.pal(name='Dark2', n=2),
legend=rownames(genders),
labels=T)
knitr::opts_chunk$set(echo = TRUE)
# RColorBrewer provides us with many beautiful colors
library(RColorBrewer)
# dplyr package will help us when it comes to data manipulation
library(dplyr)
# purrr is a functional programming toolkit that provides new ways of dealing with loops, just like the map() function.
library(purrr)
# the cluster package provides many functions to work with clustering analysis.
library(cluster)
# Enchance our plots
library(grid)
library(gridExtra)
# the NbClust package provides indices for determining the best number of clusters
library(NbClust)
# The factoextra package is a must-have when it comes to extract and visualize multivariate data analysis, such as Principal Component Analysis (PCA) and more.
library(factoextra)
customer <- read.csv('data/Mall_Customers.csv')
str(customer)
summary(customer)
head(customer)
genders <- table(customer$Gender)
barplot(genders,
main='Gender Distribution of Customers database',
xlab='Gender',
ylab='# of Customers',
col=brewer.pal(name='Dark2', n=2),
legend=rownames(genders))
hist(customer$Age,
col='steelblue',
xlab='Age',
ylab='# of Customers',
labels=TRUE)
par(mar=c(4.1, 4.1, 4.1, 2.1))
hist(customer$Age,
col='steelblue',
xlab='Age',
ylab='# of Customers',
labels=TRUE)
par(mar=c(4.1, 4.1, 4.1, 2.1))
hist(customer$Age,
col='steelblue',
xlab='Age',
ylab='# of Customers',
main= "Distribution of customers ages",
labels=TRUE)
par(mar=c(4.1, 4.1, 3.1, 2.1))
par(mar=c(4.1, 4.1, 3.1, 2.1))
hist(customer$Age,
col='steelblue',
xlab='Age',
ylab='# of Customers',
main= "Distribution of customers ages",
labels=TRUE)
par(mar=c(4.1, 4.1, 2.1, 2.1))
hist(customer$Age,
col='steelblue',
xlab='Age',
ylab='# of Customers',
main= "Distribution of customers ages",
labels=TRUE)
par(mar=c(4.1, 4.1, 1.1, 2.1))
hist(customer$Age,
col='steelblue',
xlab='Age',
ylab='# of Customers',
main= "Distribution of customers ages",
labels=TRUE)
par(mar=c(4.1, 4.1, 4.1, 2.1))
hist(customer$Age,
col='steelblue',
xlab='Age',
ylab='# of Customers',
main= "Distribution of customers ages",
labels=TRUE)
customer
customer
customer
customer
customer
# Create a function to calculate total intra-cluster sum of squares
iss <- function(k) {
kmeans(x=customer[, 3:5],
centers = k,
iter.max=100,
nstart=100,
algorithm='Lloyd')$tot.withinss
}
# Determine the number of clusters that we want to test
k.values <- 1:10
# Use the map_dbl function to map all the k.values to the function ISS
start_time <- Sys.time()
iss_values <- map_dbl(k.values, iss)
end_time <- Sys.time()
paste("The running time was: ", end_time - start_time, sep=' ')
# Plotting the results
par(mfrow=c(1,1))
plot(k.values,
iss_values,
type='b', pch=19, frame=F,
xlab='Number of Clusters K',
ylab='Total Intra-clusters sum of squares')
# The best value for the clusters number is the one that is closer to the bend of the 'elbow'
k <- 4
